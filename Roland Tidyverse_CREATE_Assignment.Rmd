---
title: "Tidyverse CREATE Assignment"
author: "Matthew Roland"
date: "2023-10-25"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyr)
library(stringr)
library(knitr)
library(ggplot2)

```

## Loading the dataset

The dataset sourced for this assignment contains data related to stroke diagnoses and stroke predictors. These data were provided by kaggle.com (https://www.kaggle.com/datasets/teamincribo/stroke-prediction/)

```{r}
stroke_pred <- read.csv("https://raw.githubusercontent.com/Mattr5541/DATA_607_CREATE/main/stroke_prediction_dataset.csv")

colnames(stroke_pred)
```
##Using tidyR

As we can see, the dataset is both clean and "tidy" in the sense that every column is a variable, and predictors are laid out (depending on the research question, we can likely collapse certain columns into each other so they may act as clearer predictors, but that will not be the main focus of this vignette)

Although these data are neatly composed and analyzable for the most part, if we look at the "Symptoms" variable, we can see that each observation contains a string composed of words broken up by commas that represent separate symptoms. Such data may be difficult to analyze in such a format, and may require modifications. Fortunately, we can accomplish this through the use of packages such as **tidyr.** The goal will be to first partition these symptoms into separate columns in accordance with the demarcating commas. Following this step, I will use **tidyr's** pivot_longer() function to condense the symptoms into a single, long column 

First, I will use **tidyr's** separate() function to split the columns based on the presence of a comma within the strings
```{r}
##First, I need to determine the maximum number of symptoms in this column

symp <- max(sapply(strsplit(stroke_pred$Symptoms, ","), length))

print(symp)

##And now we know that there is a maximum of 5 symptoms within this column


##The following code will separate the Symptoms column into 5 separate columns by calling Tidyr's separate() function. The arguments within the paste() function will generate five columns named Symptom_1 -- Symptom_5. The sep = "_" argument will ensure that the newly created symptom variables are all one word, which will be more convenient when I collapse them into a long column.
##The sep = "," argument will separate the symptoms into their respective columns when a comma is detected. Of course, this will result in a multitude of empty cells, since most observations only contained around 2-3 symptoms. However, these empty cells will be dealt with once I collapse the columns.

stroke_sep <- stroke_pred %>% separate(Symptoms, into = paste("Symptom", 1:5, sep = "_"), sep = ", " , remove = F)
```

##Using pivot_longer() to combine the Symptom columns

Now that we have separated the Symptoms column into multiple columns, our data are almost ready for analysis. However, the array of missing observations and separate predictors should be altered before we decide to analyze our data. Specifically, it would make sense in most cases to collapse the newly created Symptom columns into a single column, since these symptoms may act as appropriate outcomes for variables such as `Average.Glucose.Level` or `Body.Mass.Index.BMI`. Alternatively, these symtptoms can act as predictors for the `Diagnosis` variable

```{r}
##First, there were some blank columns that did not become NA, so I need to implement a simple ifelse() statement to fix them

stroke_sep$Symptom_1 <- ifelse(stroke_sep$Symptom_1 == "", NA, stroke_sep$Symptom_1)

##it is necessary to drop the symptoms column

stroke_sep <- stroke_sep %>% subset(select = -c(Symptoms))

stroke_long <- stroke_sep %>% pivot_longer(cols = starts_with("Symptom"), names_to = NULL, values_to = "Symptoms", values_drop_na = T)
```

And with that, our data are considerably tidier, and ready for some basic analyses!

##Analyses

Let's find some averages and make some visual representations

```{r}
unique(stroke_long$Symptoms)

stroke_long <- stroke_long %>% mutate(Diagnosis_num = if_else(Diagnosis == "Stroke", 1, 0))

stroke_long %>% summarize(neg = sum(Diagnosis_num == 0) / nrow(stroke_long), pos = sum(Diagnosis_num == 1) / nrow(stroke_long))

stroke_long %>% group_by(Symptoms) %>% summarize(means = mean(Diagnosis_num))
```

```{r}
stroke_sum <- stroke_long %>% group_by(Symptoms) %>% summarize(neg = sum(Diagnosis_num == 0) / nrow(stroke_long), pos = sum(Diagnosis_num == 1) / nrow(stroke_long))

stroke_sum <- stroke_sum %>% pivot_longer(cols = c(neg, pos), names_to = "Diagnosis", values_to = "Prop")

ggplot(stroke_sum, aes(fill = Diagnosis, x = Symptoms, y = Prop)) + geom_bar(position = "stack", stat = "identity") +  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Based on the charts and graph above, we can see that there is an almost 50/50 ratio of participants who have received a stroke diagnosis, and participants who have not. This further extends to symptom composition, which shows that each symptom has an almost 50/50 ratio of individuals who did or did not have a stroke. However, we can see that there is a slightly higher ratio of individuals diagnosed with strokes if they experiences seizures or dizziness. In contrast, those who had difficulty speaking or confusion were also marginally less likely to be diagnosed with a stroke. 

These data indicate that, individually, these symptom categories may not be the best predictors for receiving a stroke diagnosis (and I am using the term "predictor" loosely, since we have not performed any statistical analyses). This is likely because of how interconnected these symptoms are. This analysis could potentially be improved by grouping the symptoms into clusters by using machine learning procedures such as natural language processing.

Now let's try using Symptoms as an outcome

```{r}
stroke_long <- stroke_long %>% mutate(Physical_num = recode(Physical.Activity, "Low" = 1, 
                                                          "Moderate" = 2, 
                                                          "High" = 3))

stroke_long %>% group_by(Physical_num) %>% summarize()
```

